---
- name: Initialize kubernetes cluster on first control plane node
  hosts: k8s_first_control_plane
  vars_files:
    - "{{ tf_workspace }}.vars.yml"
  become: true
  tasks:
    - name: Ensure python3-kubernetes is installed (for Ansible)
      ansible.builtin.apt:
        name:
          - python3-kubernetes
    - name: Test for kubeconfig
      ansible.builtin.stat:
        path: /etc/kubernetes/admin.conf
      register: kubeconfig
    - name: Initialize cluster
      block:
        # - name: Temporarily point VIP hostname to localhost
        #   ansible.builtin.lineinfile:
        #     path: /etc/hosts
        #     line: "{{ ansible_default_ipv4.address }}  {{ vip_hostname }}.{{ ansible_domain }} {{ vip_hostname }}"
        - name: Ensure cluster is initialized (up to 5 minutes)
          ansible.builtin.command: kubeadm init --token "{{ k8s_token }}" --certificate-key "{{ k8s_certificate_key }}" --control-plane-endpoint "{{ vip_hostname }}" --apiserver-cert-extra-sans "{{ vip_hostname }}" --upload-certs
          args:
            creates: /etc/kubernetes/admin.conf
          register: kubeadm_init
        - debug:
            var: kubeadm_init.stdout_lines
        # - name: Remove temporary hosts entry for VIP
        #   ansible.builtin.lineinfile:
        #     path: /etc/hosts
        #     state: absent
        #     line: "{{ ansible_default_ipv4.address }}  {{ vip_hostname }}.{{ ansible_domain }} {{ vip_hostname }}"
        # The next two tasks exist because the kubeconfig argument for the Ansible
        # kubernetes.core.k8s* modules doesn't seem to work (at least for me)
        - name: Ensure kubeconfig directory for root exists
          ansible.builtin.file:
            path: /root/.kube
            state: directory
            owner: root
            group: root
            mode: '0700'
        - name: Ensure kubeconfig for root exists
          ansible.builtin.copy:
            src: /etc/kubernetes/admin.conf
            dest: /root/.kube/config
            remote_src: yes
            owner: root
            group: root
            mode: '0600'
      when: not kubeconfig.stat.exists
    - name: Install Calico CNI
      block:
        # - name: Temporarily point VIP hostname to localhost
        #   ansible.builtin.lineinfile:
        #     path: /etc/hosts
        #     line: "{{ ansible_default_ipv4.address }}  {{ vip_hostname }}.{{ ansible_domain }} {{ vip_hostname }}"
        - name: Install Calico CNI
          ansible.builtin.command: kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
        # - name: Remove temporary hosts entry for VIP
        #   ansible.builtin.lineinfile:
        #     path: /etc/hosts
        #     state: absent
        #     line: "{{ ansible_default_ipv4.address }}  {{ vip_hostname }}.{{ ansible_domain }} {{ vip_hostname }}"
      when:
        - use_calico_cni
        - not kubeconfig.stat.exists
    - name: Install Cilium CNI
      block:
        - name: Ensure Cilium CLI tool is installed
          ansible.builtin.shell: |
            CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
            CLI_ARCH=amd64
            if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
            curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
            sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
            sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
            rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
            CILIUM_VERSION=$(cilium version | awk '/stable/ {print $4;}')
            cilium install --version $CILIUM_VERSION
          args:
            creates: /usr/local/bin/cilium
            executable: /usr/bin/bash
          register: cilium
        - debug:
            var: cilium.stdout_lines
        - name: Wait for Cilium to be OK
          ansible.builtin.command: cilium status --wait
          register: cilium_wait
          until: cilium_wait is success
          retries: 3
          delay: 10
      when:
        - use_cilium_cni
        - not kubeconfig.stat.exists
      # Retries are here to handle when connections are lost during etcd failovers.
      # Why these are frequently occurring still needs troubleshooting.
    - name: Wait for initial control plane node to be ready
      ansible.builtin.command: kubectl wait --for=condition=Ready nodes {{ ansible_hostname }} --timeout=600s
      register: nodes_ready
      retries: 10
      delay: 5
    - debug:
        var: nodes_ready.stdout_lines
